{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3105a4f2",
   "metadata": {},
   "source": [
    "# Prototype 2 for Sentiment Analysis with full scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fe22a",
   "metadata": {},
   "source": [
    "This is an improvement from the first prototype for the Sentiment analysis. In this version, we will train using the entire dataset. The notebook assumes the [Split dataest into multiple files.ipynb](Split%20dataset%20into%20multiple%20files.ipynb) notebook was executed to split the dataset into multiple files of 10000 lines each. The training will do mini-batch training using the split chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c06a3",
   "metadata": {},
   "source": [
    "### Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c10001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e22265",
   "metadata": {},
   "source": [
    "### Load database required for removing stopword and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16360f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\siva.alagarsamy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\siva.alagarsamy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\siva.alagarsamy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Downloads all english dictionary words\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77635c56",
   "metadata": {},
   "source": [
    "### Define a function to divide the large dataset into multiple chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62a3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(txt_bz_file, output_path=\".\", ):\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "        \n",
    "    file_name = txt_bz_file.split(\"/\")[-1]\n",
    "    file_name_prefix = file_name.split(\".\")[0]\n",
    "    row_number = 0\n",
    "    chunk = 1\n",
    "    out_file_name = f\"{output_path}/{file_name_prefix}_{chunk}.txt.bz2\"\n",
    "    out_file = bz2.open(out_file_name, \"wt\", encoding='utf-8')\n",
    "    \n",
    "    with bz2.open(txt_bz_file, \"rt\", encoding='utf-8') as bz_file:\n",
    "        for line in bz_file:\n",
    "            out_file.write(line)\n",
    "            row_number += 1\n",
    "            if row_number == 10000 :\n",
    "                out_file.close()\n",
    "                chunk += 1\n",
    "                out_file_name = f\"{output_path}/{file_name_prefix}_{chunk}.txt.bz2\"\n",
    "                out_file = bz2.open(out_file_name, \"wt\", encoding='utf-8')\n",
    "                row_number = 0\n",
    "    out_file.close()\n",
    "    \n",
    "    # If the last file was an empty file, delete it. \n",
    "    if row_number == 0 :\n",
    "        os.remove(out_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f10c8",
   "metadata": {},
   "source": [
    "### Divide the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672303b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file('data/train.ft.txt.bz2', output_path='data/train_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb06ec7",
   "metadata": {},
   "source": [
    "### Divide the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e344a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file('data/test.ft.txt.bz2', output_path='data/test_chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b309a2",
   "metadata": {},
   "source": [
    "### Define a function to normalize words in a sentence\n",
    "We do the following\n",
    "+ Convert all words to lower case, so we are doing not analyzing words with different case as different words\n",
    "+ Drop any stop words like I, me, this, is ...\n",
    "+ Remove words that are not in english dictionary. \n",
    "+ Remove punctuations\n",
    "+ Lemmatize words. This is converting different forms of a word to a base form.  E.g convert word like caring to care, bats to bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bafb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"!@#$%^&*()_-+={[}]|\\:;'<,>.?/~`\"\n",
    "\n",
    "def to_words(text):\n",
    "    words = []\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    for w in tokens:\n",
    "        # Convert to lower\n",
    "        w = w.lower()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        w = \"\".join([char for char in w if char not in punctuations])\n",
    "        \n",
    "        # Don't add word if it is a stopword\n",
    "        if w not in stop_words:      \n",
    "            \n",
    "            # Make sure it is valid english word\n",
    "            if w in english_words:\n",
    "                # Lemmatize word\n",
    "                w = lemmatizer.lemmatize(w, 'v')  #Assume most of the review is verb part of the speech (POS)\n",
    "                words.append(w)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47172a3",
   "metadata": {},
   "source": [
    "### Define a function that will load the reviews file and convert it to normalized words and return the sentiment labels and words as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ba4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(txt_bz_file):\n",
    "    sentiments = []\n",
    "    reviews = []\n",
    "    \n",
    "    with bz2.open(txt_bz_file, \"rt\", encoding='utf-8') as bz_file:\n",
    "        for line in bz_file:\n",
    "            # Label and review are separated by space\n",
    "            label, review = line.split(' ', maxsplit=1)\n",
    "            \n",
    "            # label has a format __label__2  we just need the last number\n",
    "            sentiments.append(int(label[9:]))\n",
    "            \n",
    "            # The title and the body are separated by :, so we split them \n",
    "            title, body = review.split(':', maxsplit=1)\n",
    "            \n",
    "            title_part = \" \".join(to_words(title))\n",
    "            body_part = \" \".join(to_words(body))\n",
    "            \n",
    "            sentence = \" \".join([title_part, body_part])\n",
    "            reviews.append(sentence)\n",
    "    return sentiments, reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eccef1",
   "metadata": {},
   "source": [
    "### Define methods for TfidfVectorizer to be updated over multiple dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13a272",
   "metadata": {},
   "source": [
    "Sklearn doesn't provide a partial_fit method fo TfidfVectorizer, but someone has a patch that will allow the TfidfVectorizer to be updated in batch.  [Here is the link to the Stackoverflow page](https://stackoverflow.com/questions/39109743/adding-new-text-to-sklearn-tfidif-vectorizer-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfc9ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse.dia import dia_matrix\n",
    "\n",
    "def tfidf_first_fit(self, X):\n",
    "    self.fit(X)\n",
    "    self.n_docs = len(X)\n",
    "    \n",
    "def tfidf_update_fit(self, X):\n",
    "    max_idx = max(self.vocabulary_.values())\n",
    "    for a in X:\n",
    "        #update vocabulary_\n",
    "        if self.lowercase: a = a.lower()\n",
    "        tokens = re.findall(self.token_pattern, a)\n",
    "        for w in tokens:\n",
    "            if w not in self.vocabulary_:\n",
    "                max_idx += 1\n",
    "                self.vocabulary_[w] = max_idx\n",
    "\n",
    "        #update idf_\n",
    "        df = (self.n_docs + self.smooth_idf)/np.exp(self.idf_ - 1) - self.smooth_idf\n",
    "        self.n_docs += 1\n",
    "        df.resize(len(self.vocabulary_))\n",
    "        for w in tokens:\n",
    "            df[self.vocabulary_[w]] += 1\n",
    "        idf = np.log((self.n_docs + self.smooth_idf)/(df + self.smooth_idf)) + 1\n",
    "        self._tfidf._idf_diag = dia_matrix((idf, 0), shape=(len(idf), len(idf)))\n",
    "\n",
    "TfidfVectorizer.first_fit = tfidf_first_fit\n",
    "TfidfVectorizer.update_fit = tfidf_update_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa87c9",
   "metadata": {},
   "source": [
    "### Fit the TfidVectorizer on all of the train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e72fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, min_df=5)\n",
    "\n",
    "# Go over the training chunck files 1 - 360\n",
    "for chunk in range(1,361):\n",
    "    file_name = f\"data/train_chunks/train_{chunk}.txt.bz2\"\n",
    "    sentiments, reviews = load_data(file_name)\n",
    "    \n",
    "    if chunk == 1 :\n",
    "        tfidf_vectorizer.first_fit(reviews)\n",
    "    else:\n",
    "        tfidf_vectorizer.update_fit(reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed54c2",
   "metadata": {},
   "source": [
    "### Train SGDClassifier using partial_fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ca60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = SGDClassifier(max_iter=5000, random_state=7)\n",
    "\n",
    "# Go over the training chunck files 1 - 360\n",
    "for chunk in range(1,361):\n",
    "    file_name = f\"data/train_chunks/train_{chunk}.txt.bz2\"\n",
    "    sentiments, reviews = load_data(file_name)\n",
    "    train_tfidf = tfidf_vectorizer.transform(reviews)\n",
    "    train_df = pd.DataFrame(train_tfidf.toarray(), \n",
    "                            columns=tfidf_vectorizer.get_feature_names())\n",
    "    \n",
    "    if chunk == 1 :\n",
    "        # classes are 1 & 2, 1 = Bad, 2 = Good\n",
    "        clf.partial_fit(train_df, sentiments, classes=[1, 2])\n",
    "    else :\n",
    "        clf.partial_fit(train_df, sentiments)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68f14d",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cf22f",
   "metadata": {},
   "source": [
    "Test the model by going through the test chucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1159809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy =  0.8690049999999999\n",
      "Average F1 Score =  0.8684925138756938\n",
      "Confusion Matrix =  [[173091  26909]\n",
      " [ 25489 174511]]\n"
     ]
    }
   ],
   "source": [
    "# Go over the test chunck files\n",
    "\n",
    "accuracy_sum = 0\n",
    "f1_score_sum = 0\n",
    "conf_sum = [[0, 0], [0,0]]\n",
    "num_chunks = 0\n",
    "\n",
    "result_file = open(\"Prototype2_result.txt\", \"w\")\n",
    "\n",
    "# Chunks 1 - 40\n",
    "for chunk in range(1,41):\n",
    "    file_name = f\"data/test_chunks/test_{chunk}.txt.bz2\"\n",
    "    sentiments, reviews = load_data(file_name)\n",
    "    test_tfidf = tfidf_vectorizer.transform(reviews)\n",
    "    test_df = pd.DataFrame(test_tfidf.toarray(), \n",
    "             columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "    sentiments_predicted = clf.predict(test_df)\n",
    "\n",
    "    accuracy = accuracy_score(sentiments, sentiments_predicted)\n",
    "    confusion = confusion_matrix(sentiments, sentiments_predicted)\n",
    "    f1 = f1_score(sentiments, sentiments_predicted)\n",
    "    \n",
    "    accuracy_sum += accuracy\n",
    "    f1_score_sum += f1\n",
    "    conf_sum += confusion\n",
    "    num_chunks += 1\n",
    "    \n",
    "    # Print accuracy score and confusion matrix\n",
    "    result_file.write(f'{file_name}\\r\\n')\n",
    "    result_file.write(f'Accuracy score  = {accuracy}\\r\\n')\n",
    "    result_file.write(f'Confusion Matrix = {confusion}\\r\\n')\n",
    "    result_file.write(f'F1 Score = {f1}\\r\\n' )\n",
    "    result_file.write('----------------------------------------------------\\r\\n')\n",
    "\n",
    "accuracy_avg = accuracy_sum / num_chunks\n",
    "f1_avg = f1_score_sum / num_chunks\n",
    "\n",
    "result_file.write(f'Average Acuuracy = {accuracy_avg}\\r\\n')\n",
    "result_file.write(f'Average F1 Score = {f1_avg}\\r\\n')\n",
    "result_file.write(f'Confusion Matrix = {conf_sum}\\r\\n')\n",
    "result_file.close()\n",
    "\n",
    "print('Average Accuracy = ', accuracy_avg)\n",
    "print('Average F1 Score = ', f1_avg)\n",
    "print('Confusion Matrix = ', conf_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f62d4",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f851c",
   "metadata": {},
   "source": [
    "The prototype has an accuracy of 86%. This will be improved using deep learning in the later iteration of the prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98372058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
