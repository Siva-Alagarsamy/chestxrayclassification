{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3105a4f2",
   "metadata": {},
   "source": [
    "# Benchmark Various Models for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fe22a",
   "metadata": {},
   "source": [
    "This notebook benchmarks Logistic regression, Random Forest and XGBoost models for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c06a3",
   "metadata": {},
   "source": [
    "### Define the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c10001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e22265",
   "metadata": {},
   "source": [
    "### Load database required for removing stopword and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16360f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Siva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Siva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Siva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Downloads all english dictionary words\n",
    "nltk.download('words')\n",
    "english_words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b309a2",
   "metadata": {},
   "source": [
    "### Define a function to normalize words in a sentence\n",
    "We do the following\n",
    "+ Convert all words to lower case, so we are doing not analyzing words with different case as different words\n",
    "+ Drop any stop words like I, me, this, is ...\n",
    "+ Remove words that are not in english dictionary. \n",
    "+ Remove punctuations\n",
    "+ Lemmatize words. This is converting different forms of a word to a base form.  E.g convert word like caring to care, bats to bat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bafb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"!@#$%^&*()_-+={[}]|\\:;'<,>.?/~`\"\n",
    "\n",
    "def to_words(text):\n",
    "    words = []\n",
    "    tokens = re.findall('\\w+', text)\n",
    "    for w in tokens:\n",
    "        # Convert to lower\n",
    "        w = w.lower()\n",
    "        \n",
    "        # Remove punctuations\n",
    "        w = \"\".join([char for char in w if char not in punctuations])\n",
    "        \n",
    "        # Don't add word if it is a stopword\n",
    "        if w not in stop_words:      \n",
    "            \n",
    "            # Make sure it is valid english word\n",
    "            if w in english_words:\n",
    "                # Lemmatize word\n",
    "                w = lemmatizer.lemmatize(w, 'v')  #Assume most of the review is verb part of the speech (POS)\n",
    "                words.append(w)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47172a3",
   "metadata": {},
   "source": [
    "### Define a function that will load the reviews file and convert it to normalized words and return the sentiment labels and words as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ba4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(txt_bz_file):\n",
    "    sentiments = []\n",
    "    reviews = []\n",
    "    \n",
    "    with bz2.open(txt_bz_file, \"rt\", encoding='utf-8') as bz_file:\n",
    "        for line in bz_file:\n",
    "            # Label and review are separated by space\n",
    "            label, review = line.split(' ', maxsplit=1)\n",
    "            \n",
    "            # label has a format __label__2  we just need the last number\n",
    "            sentiments.append(int(label[9:]))\n",
    "            \n",
    "            # The title and the body are separated by :, so we split them \n",
    "            title, body = review.split(':', maxsplit=1)\n",
    "            \n",
    "            title_part = \" \".join(to_words(title))\n",
    "            body_part = \" \".join(to_words(body))\n",
    "            \n",
    "            sentence = \" \".join([title_part, body_part])\n",
    "            reviews.append(sentence)\n",
    "    return sentiments, reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa87c9",
   "metadata": {},
   "source": [
    "### Load the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e72fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiments, train_reviews = load_data('data/train.ft.txt.bz2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed54c2",
   "metadata": {},
   "source": [
    "### Load the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5ca60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments, test_reviews = load_data('data/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e1f26",
   "metadata": {},
   "source": [
    "### Do count vectorization and create a dataframe for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d7d28c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 973. GiB for an array with shape (3600000, 36290) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cdb7ea74831d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrain_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m train_df = pd.DataFrame(train_tfidf.toarray(), \n\u001b[0m\u001b[0;32m     15\u001b[0m              columns=count_vect.get_feature_names())\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 973. GiB for an array with shape (3600000, 36290) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# max_df=0.85 - Ignore words that occur in 85% of the reviews. They are not going to help and are usually words like \"the\"\n",
    "# min_df=5 - Ignore words that happen less than 5 times in the entire dataset. Since they are very rare, they won't be of any use\n",
    "count_vect = CountVectorizer(max_df=0.85, min_df=5)  \n",
    "\n",
    "# We need the vectorizer to account for all words that may only exists in test data, so lets fit the vectorizer with all words f\n",
    "count_vect.fit(train_reviews + test_reviews)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "\n",
    "train_counts = count_vect.transform(train_reviews)\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)\n",
    "train_df = pd.DataFrame(train_tfidf.toarray(), \n",
    "             columns=count_vect.get_feature_names())\n",
    "\n",
    "test_counts = count_vect.transform(test_reviews)\n",
    "test_tfidf = tfidf_transformer.fit_transform(test_counts)\n",
    "test_df = pd.DataFrame(test_tfidf.toarray(), \n",
    "             columns=count_vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a68f14d",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cf22f",
   "metadata": {},
   "source": [
    "Build a LogisticRegression model and see how well it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1159809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "clf = LogisticRegression()\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(train_df, train_sentiments)\n",
    "\n",
    "# Predict\n",
    "test_sentiments_predicted = clf.predict(test_df)\n",
    "\n",
    "# Print accuracy score and confusion matrix\n",
    "\n",
    "print('Accuracy score of LogisticRegression = ', accuracy_score(test_sentiments, test_sentiments_predicted))\n",
    "\n",
    "print('Confusion Matrix for LogisticRegression')\n",
    "print(confusion_matrix(test_sentiments, test_sentiments_predicted))\n",
    "print('F1 Score = ', f1_score(test_sentiments, test_sentiments_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880e18",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f3362",
   "metadata": {},
   "source": [
    "Build a RandomForest model and see how well it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#hyper_params = { \n",
    "#    'n_estimators': [16, 32, 64, 128, 256, 500],\n",
    "#    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#    'max_depth' : [10, 15, 20, 30, 40],\n",
    "#    'criterion' :['gini', 'entropy']\n",
    "#}\n",
    "\n",
    "#rfc = GridSearchCV(RandomForestClassifier(), hyper_params, cv= 5)\n",
    "#print(\"Best Parameters: {}\".format(rfc.best_params_))\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=128, max_features='sqrt', max_depth=30, criterion='gini')\n",
    "rfc.fit(train_df, train_sentiments)\n",
    "test_sentiments_predicted = rfc.predict(test_df)\n",
    "\n",
    "print(\"Acuracy Score for RandomForest = \", accuracy_score(test_sentiments, test_sentiments_predicted))\n",
    "print('Confusion Matrix for RandomForest')\n",
    "print(confusion_matrix(test_sentiments, test_sentiments_predicted))\n",
    "print('F1 Score = ', f1_score(test_sentiments, test_sentiments_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9238ec6b",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8174f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(train_df, train_sentiments)\n",
    "test_sentiments_predicted = rfc.predict(test_df)\n",
    "\n",
    "print(\"Acuracy Score for XGBoost = \", accuracy_score(test_sentiments, test_sentiments_predicted))\n",
    "print('Confusion Matrix for XGBoost')\n",
    "print(confusion_matrix(test_sentiments, test_sentiments_predicted))\n",
    "print('F1 Score = ', f1_score(test_sentiments, test_sentiments_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a16c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
